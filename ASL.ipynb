{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ddc8b8c-d63b-4018-a8e3-abefb71373bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models, transforms, datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "# from utils import *\n",
    "from model import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d9da497-98f2-45ef-8fd7-c3d5c690d3e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_path = '/mnt/c/Dataset/asl-alphabet/asl_alphabet_train/'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48370d39-70cc-4868-8f91-2f8a05dbf0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66dab6d8-0455-42e1-866a-9848dd6af9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.3 ms, sys: 24.5 ms, total: 87.7 ms\n",
      "Wall time: 460 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = datasets.ImageFolder(train_data_path, transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2727e2b-e9ad-490e-87c3-eec1fbc02e45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.8 ms, sys: 32 ms, total: 92.8 ms\n",
      "Wall time: 606 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_dataset = datasets.ImageFolder(train_data_path, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8648ddfa-16cf-4cfa-8ff8-04ab5db8c005",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69600, 17400)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "torch.manual_seed(time())\n",
    "num_train_samples = len(train_dataset)\n",
    "# num_train_samples = 20000\n",
    "\n",
    "val_split = 0.2\n",
    "split = int(num_train_samples * val_split)\n",
    "indices = torch.randperm(num_train_samples)\n",
    "\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_dataset, indices[split:])\n",
    "val_subset = torch.utils.data.Subset(val_dataset, indices[:split])\n",
    "\n",
    "len(train_subset), len(val_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2ee8bcc-405f-46fe-a70b-0acf7a580c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_subset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=16\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_subset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26e481cf-072c-4fa6-bf93-44cb1ab26675",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = train_dataloader.dataset.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7552c567-32bb-4f66-871f-4d84c1a1a8d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32]) torch.Size([64])\n",
      "Ground Truth A\n",
      "torch.Size([1, 32, 32])\n",
      "torch.Size([32, 32, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img, label in train_dataloader:\n",
    "    print(img.shape, label.shape)\n",
    "    print(f'Ground Truth {classes[label[0]]}')\n",
    "    print(img[0].size())\n",
    "    print(img[0].permute(1, 2, 0).size())\n",
    "    plt.imshow(img[0].permute(1, 2, 0),cmap='gray')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfbba33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myCNN(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (fc): Linear(in_features=512, out_features=29, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model= myCNN().to(device)\n",
    "#model = torch.load('checkpoints/checkpoint_97.74.pth', map_location='cpu').to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b10360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8216290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Full Model Accuracy: 2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def full_inference(model, test_loader):\n",
    "    correct = 0\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for i, (data, target) in enumerate(test_loader, 1):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Full Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "full_inference(model, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dce7144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myCNN(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (fc): Linear(in_features=512, out_features=29, bias=False)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    relu = torch.nn.functional.relu(conv1, inplace = False);  conv1 = None\n",
      "    max_pool2d = torch.nn.functional.max_pool2d(relu, 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False, return_indices = False);  relu = None\n",
      "    conv2 = self.conv2(max_pool2d);  max_pool2d = None\n",
      "    relu_1 = torch.nn.functional.relu(conv2, inplace = False);  conv2 = None\n",
      "    max_pool2d_1 = torch.nn.functional.max_pool2d(relu_1, 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False, return_indices = False);  relu_1 = None\n",
      "    view = max_pool2d_1.view(-1, 512);  max_pool2d_1 = None\n",
      "    fc = self.fc(view);  view = None\n",
      "    return fc\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "myCNN(\n",
      "  (conv1): QConv2d(\n",
      "    (qi): QParam()\n",
      "    (qo): QParam()\n",
      "    (conv_module): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (qw): QParam()\n",
      "  )\n",
      "  (conv2): QConv2d(\n",
      "    (qo): QParam()\n",
      "    (conv_module): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (qw): QParam()\n",
      "  )\n",
      "  (fc): QLinear(\n",
      "    (qo): QParam()\n",
      "    (fc_module): Linear(in_features=512, out_features=29, bias=False)\n",
      "    (qw): QParam()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    relu = torch.nn.functional.relu(conv1, inplace = False);  conv1 = None\n",
      "    max_pool2d = torch.nn.functional.max_pool2d(relu, 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False, return_indices = False);  relu = None\n",
      "    conv2 = self.conv2(max_pool2d);  max_pool2d = None\n",
      "    relu_1 = torch.nn.functional.relu(conv2, inplace = False);  conv2 = None\n",
      "    max_pool2d_1 = torch.nn.functional.max_pool2d(relu_1, 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False, return_indices = False);  relu_1 = None\n",
      "    view = max_pool2d_1.view(-1, 512);  max_pool2d_1 = None\n",
      "    fc = self.fc(view);  view = None\n",
      "    return fc\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "myCNN(\n",
      "  (conv1): QConv2d(\n",
      "    (qi): QParam()\n",
      "    (qo): QParam()\n",
      "    (conv_module): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (qw): QParam()\n",
      "  )\n",
      "  (conv2): QConv2d(\n",
      "    (qo): QParam()\n",
      "    (conv_module): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (qw): QParam()\n",
      "  )\n",
      "  (fc): QLinear(\n",
      "    (qo): QParam()\n",
      "    (fc_module): Linear(in_features=512, out_features=29, bias=False)\n",
      "    (qw): QParam()\n",
      "  )\n",
      "  (qrelu_2): QReLU(\n",
      "    (qo): QParam()\n",
      "  )\n",
      "  (qmaxpool2d_3): QMaxPooling2d(\n",
      "    (qo): QParam()\n",
      "  )\n",
      "  (qrelu_5): QReLU(\n",
      "    (qo): QParam()\n",
      "  )\n",
      "  (qmaxpool2d_6): QMaxPooling2d(\n",
      "    (qo): QParam()\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    qrelu_2 = self.qrelu_2(conv1, inplace = False);  conv1 = None\n",
      "    qmaxpool2d_3 = self.qmaxpool2d_3(qrelu_2, 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False, return_indices = False);  qrelu_2 = None\n",
      "    conv2 = self.conv2(qmaxpool2d_3);  qmaxpool2d_3 = None\n",
      "    qrelu_5 = self.qrelu_5(conv2, inplace = False);  conv2 = None\n",
      "    qmaxpool2d_6 = self.qmaxpool2d_6(qrelu_5, 2, stride = 2, padding = 0, dilation = 1, ceil_mode = False, return_indices = False);  qrelu_5 = None\n",
      "    view = qmaxpool2d_6.view(-1, 512);  qmaxpool2d_6 = None\n",
      "    fc = self.fc(view);  view = None\n",
      "    return fc\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n",
      "Quantization bit: 4\n"
     ]
    }
   ],
   "source": [
    "num_bits = 4\n",
    "\n",
    "model.quantize(num_bits=num_bits)\n",
    "\n",
    "print('Quantization bit: %d' % num_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "987fdbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myCNN(\n",
       "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (fc): Linear(in_features=512, out_features=29, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f99fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model,\n",
    "          criterion,\n",
    "          optimizer,\n",
    "          train_dataloader,\n",
    "          test_dataloader,\n",
    "          print_every,\n",
    "          num_epoch):\n",
    "    steps = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(num_epoch)):\n",
    "        running_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        start_time = time()\n",
    "        iter_time = time()\n",
    "        \n",
    "        #model.train()\n",
    "        for i, (images, labels) in enumerate(train_dataloader):\n",
    "            steps += 1\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images_copy = images.clone().detach()\n",
    "\n",
    "            # Forward pass\n",
    "            \n",
    "            output = model(images_copy)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            correct_train += (torch.max(output, dim=1)[1] == labels).type(torch.float).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Logging\n",
    "            if steps % print_every == 0:\n",
    "                print(f'Epoch [{epoch + 1}]/[{num_epoch}]. Batch [{i + 1}]/[{len(train_dataloader)}].', end=' ')\n",
    "                print(f'Train loss {running_loss / steps:.5f}.', end=' ')\n",
    "                print(f'Train acc {correct_train / total_train * 100:.5f}.', end=' ')\n",
    "                with torch.no_grad():\n",
    "                    # model.eval()\n",
    "                    correct_val, total_val = 0, 0\n",
    "                    val_loss = 0\n",
    "                    for images, labels in test_dataloader:\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        output = model(images)\n",
    "                        loss = criterion(output, labels)\n",
    "                        val_loss += loss.item()\n",
    "\n",
    "                        correct_val += (torch.max(output, dim=1)[1] == labels).type(torch.float).sum().item()\n",
    "                        total_val += labels.size(0)\n",
    "\n",
    "                print(f'Val loss {val_loss / len(test_dataloader):.5f}. Val acc {correct_val / total_val * 100:.5f}.', end=' ')\n",
    "                print(f'Took {time() - iter_time:.5f} seconds')\n",
    "                iter_time = time()\n",
    "\n",
    "                train_losses.append(running_loss / total_train)\n",
    "                val_losses.append(val_loss / total_val)\n",
    "        scheduler.step(val_loss / len(test_dataloader))\n",
    "\n",
    "\n",
    "        print(f'Epoch took {time() - start_time}') \n",
    "        torch.save(model, f'checkpoints/checkpoint_{correct_val / total_val * 100:.2f}.pth')\n",
    "        \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b251b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1]/[100]. Batch [50]/[1088]. Train loss 3.36396. Train acc 4.12500. Val loss 3.34768. Val acc 3.43678. Took 13.94349 seconds\n",
      "Epoch [1]/[100]. Batch [100]/[1088]. Train loss 3.34470. Train acc 4.84375. Val loss 3.29074. Val acc 9.13793. Took 12.22381 seconds\n",
      "Epoch [1]/[100]. Batch [150]/[1088]. Train loss 3.29807. Train acc 7.72917. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:37<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m num_epoch \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#model = torch.load(\"checkpoints/checkpoint_85.68.pth\", map_location='cpu')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#model = model.to(device)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model, train_losses, val_losses \u001b[39m=\u001b[39m train(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     print_every\u001b[39m=\u001b[39;49mprint_every,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     num_epoch\u001b[39m=\u001b[39;49mnum_epoch\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "\u001b[1;32m/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m output \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmyrdp/home/rich_tang/DeepLearningProgramming/ASL/ASL.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m val_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/DeepLearningProgramming/ASL/model.py:36\u001b[0m, in \u001b[0;36mmyCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x))\n\u001b[1;32m     37\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(x, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     38\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_every = 50\n",
    "num_epoch = 100\n",
    "#model = torch.load(\"checkpoints/checkpoint_85.68.pth\", map_location='cpu')\n",
    "#model = model.to(device)\n",
    "\n",
    "model, train_losses, val_losses = train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=val_dataloader,\n",
    "    print_every=print_every,\n",
    "    num_epoch=num_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e5301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_losses_array=np.array(train_losses)\n",
    "val_losses_array=np.array(val_losses)\n",
    "np.save('train_losses_array.npy',train_losses_array) \n",
    "np.save('val_losses_array.npy',val_losses_array) \n",
    "# np.save('train_losses_array.txt','w',train_losses) \n",
    "# np.save('val_losses_array.txt','w',val_losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effd7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "test_data_path = Path('/mnt/d/Dataset/asl-alphabet/asl_alphabet_test')\n",
    "\n",
    "\n",
    "class ASLTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_path, transforms=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.imgs = sorted(list(Path(root_path).glob('*.jpg')))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        \n",
    "        label = img_path.parts[-1].split('_')[0]\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d043e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = ASLTestDataset(test_data_path, transforms=test_transforms)\n",
    "\n",
    "columns = 7\n",
    "row = round(len(test_dataset) / columns) + 1\n",
    "\n",
    "fig, ax = plt.subplots(row, columns, figsize=(columns * row, row * columns))\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "\n",
    "test_model = torch.load(\"checkpoints/checkpoint_96.89.pth\", map_location='cpu')\n",
    "#test_model = model\n",
    "test_model.to(device)\n",
    "\n",
    "i, j = 0, 0\n",
    "for img, label in test_dataset:\n",
    "    img = torch.Tensor(img)\n",
    "    img = img.to(device)\n",
    "    test_model.eval()\n",
    "    prediction = test_model(img[None])\n",
    "\n",
    "    ax[i][j].imshow(img.cpu().permute(1, 2, 0),cmap='gray')\n",
    "    ax[i][j].set_title(f'GT {label}. Pred {classes[torch.max(prediction, dim=1)[1]]}') #torch.max(prediction, dim=1)[1]\n",
    "    ax[i][j].axis('off')\n",
    "    j += 1\n",
    "    if j == columns:\n",
    "        j = 0\n",
    "        i += 1\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a41cc-80bb-479d-b46b-65c3bb6f8fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
